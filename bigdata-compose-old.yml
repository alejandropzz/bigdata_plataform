version: '3.7'

networks:
  hadoop_net:
    driver: bridge

services:
  namenode:
    container_name: namenode_container
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-namenode
    hostname: namenode
    environment:
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      CLUSTER_NAME: hadoop-cluster
      TZ: America/Mexico_City
    networks:
      - hadoop_net
    volumes:
      - ./docker/compose/configs:/opt/hadoop/etc/hadoop:ro
      - hadoop_namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"
      - "9000:9000"
      - "8020:8020"

    command: >
      bash -c "
      if [ ! -d /hadoop/dfs/name/current ]; then
        echo 'Formateando NameNode por primera vez...';
        hdfs namenode -format;
      fi;
      exec hdfs namenode"
#    command: >
#      bash -c "
#      hdfs namenode -format -force || true;
#      exec hdfs namenode"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9870/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus | grep -q 'active'"]
      interval: 10s
      timeout: 5s
      retries: 20

    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1GB

  datanode:
    container_name: datanode_container
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-datanode
    hostname: datanode
    environment:
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      CLUSTER_NAME: hadoop-cluster
      TZ: America/Mexico_City
    networks:
      - hadoop_net
    volumes:
      - ./docker/compose/configs:/opt/hadoop/etc/hadoop:ro
      - hadoop_datanode:/hadoop/dfs/data
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1GB
    # Nuevo
    command: >
      bash -c "
      while ! curl -sf http://namenode:9870/ >/dev/null; do
        echo 'Esperando a NameNode...';
        sleep 5;
      done;
      exec hdfs datanode"
    restart: unless-stopped
    depends_on:
      namenode:
        condition: service_healthy

  resourcemanager:
    container_name: resourcemanager_container
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-resourcemanager
    hostname: resourcemanager
    environment:
      SKIP_CONF_GENERATION: "true"
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      YARN_CONF_DIR: /opt/hadoop/etc/hadoop
      CLUSTER_NAME: hadoop-cluster
      TZ: America/Mexico_City
    networks:
        - hadoop_net
    entrypoint: ["bash", "-c", "exec yarn resourcemanager"]  # ðŸ‘ˆ Anula el entrypoint original
    volumes:
      - ./docker/compose/configs:/opt/hadoop/etc/hadoop:ro
      - hadoop_yarn_logs:/hadoop/yarn/logs
    ports:
      - "8088:8088"  # Web UI
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://resourcemanager:8088/ws/v1/cluster/info || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
    depends_on:
      - namenode
      - datanode
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2GB

  nodemanager:
    container_name: nodemanager_container
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-resourcemanager
    hostname: nodemanager
    environment:
      SKIP_CONF_GENERATION: "true" #Verificar
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      YARN_CONF_DIR: /opt/hadoop/etc/hadoop
      CLUSTER_NAME: hadoop-cluster
      TZ: America/Mexico_City
    networks:
      - hadoop_net
    entrypoint: ["bash", "-c", "exec yarn nodemanager"]
    volumes:
      - ./docker/compose/configs:/opt/hadoop/etc/hadoop:ro
      - hadoop_yarn_logs:/hadoop/yarn/logs
    ports:
      - "8042:8042"
    depends_on:
      - resourcemanager
    deploy:
      resources:
        limits:
          cpus: '6'
          memory: 12GB
  zoo:
    image: zookeeper:3.6
    container_name: zoo
    environment:
      TZ: America/Mexico_City
    ports:
      - "2181:2181"  # Puerto expuesto para Zookeeper
    networks:
      - hadoop_net


  kafka:
    image: wurstmeister/kafka:2.12-2.2.1
    container_name: kafka
    depends_on:
      - zoo
    ports:
      - "9092:9092"  # Mapeo para acceso externo (OUTSIDE)
      - "9093:9093"  # Opcional: solo si usas OUTSIDE en 9093
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zoo:2181
      ALLOW_PLAINTEXT_LISTENER: "yes"      
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:9093      
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:9093      
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT      
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      TZ: America/Mexico_City
    networks:
      - hadoop_net
  kafka_manager:
    image: hlebalbau/kafka-manager:2.0.0.2
    container_name: kafka_manager
    ports:
      - "9001:9000"
    environment:
      - ZK_HOSTS=zoo:2181
      - KAFKA_MANAGER_AUTH_ENABLED=true
      - KAFKA_MANAGER_USERNAME=hai
      - KAFKA_MANAGER_PASSWORD=hai
    command: -Dpidfile.path=/dev/null
    networks:
      - hadoop_net

  postgres:
    image: postgres:latest
    container_name: shopping_cart_postgres
    environment:
      - "TZ=Europe/Amsterdam"
      - "POSTGRES_USER=shopping_cart"
      - "POSTGRES_PASSWORD=shopping_cart"
      - "TZ=America/Mexico_City"
    ports:
      - "5432:5432"  # credentials (shopping_cart:shopping_cart)
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - hadoop_net


  spark:
    container_name: spark_container
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-spark
    hostname: spark
    environment:
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf  # Corregido para bitnami
      - HADOOP_CONF_DIR=/opt/bitnami/spark/conf  # Usa la misma ruta que el Dockerfile
      - YARN_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_MASTER=yarn
      - SPARK_SUBMIT_OPTIONS="--conf spark.jars.ivy=/tmp/.ivy2"  # SoluciÃ³n Ivy
      - TZ=America/Mexico_City
    networks:
      - hadoop_net
    volumes:
      - ./docker/compose/configs:/opt/bitnami/spark/conf:rw  # Ruta bitnami
      - ~/bigdata_platform/scripts:/opt/scripts
    ports:
      - "4040:4040"  # Spark UI
      - "7077:7077"  # Spark UI
    healthcheck:
      test: ["CMD-SHELL", "netstat -tuln | grep 4040 || exit 1"]  # MÃ¡s eficiente que curl
      interval: 10s
      timeout: 2s
      retries: 10
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4GB

    command: >
      bash -c "
      until ping -c1 resourcemanager &>/dev/null; do
        echo 'Esperando a ResourceManager...';
        sleep 5;
      done;

      /opt/bitnami/spark/sbin/start-master.sh &&
      /opt/bitnami/spark/sbin/start-worker.sh spark://spark:7077 &&

      echo 'Configuraciones terminadas';
      tail -f /dev/null"


  jupyter-scala:
    hostname: jupyter
    container_name: jupyter-scala
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-jupyter  # Usa tu Dockerfile personalizado
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
      - ~/.ivy2:/home/jovyan/.ivy2
      - ~/bigdata_platform/scripts:/home/jovyan/scripts
    environment:
      - SPARK_MASTER=yarn://resourcemanager:8032
      - TZ=America/Mexico_City
    depends_on:
      - spark
    networks:
      - hadoop_net



volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_yarn_logs:
  postgres_data:
  spark_logs:
