version: '3.7'

networks:
  bigdata_network:
    driver: bridge
    external: true

services:
  namenode:
    container_name: namenode_container
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-namenode
    hostname: namenode
    environment:
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      CLUSTER_NAME: hadoop-cluster
      TZ: America/Mexico_City
    networks:
      - bigdata_network
    volumes:
      - ./docker/compose/configs:/opt/hadoop/etc/hadoop:ro
      - hadoop_namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"
      - "9000:9000"
      - "8020:8020"
    command: >
      bash -c "
      if [ ! -d /hadoop/dfs/name/current ]; then
        echo 'Formateando NameNode por primera vez...';
        hdfs namenode -format;
      fi;
      exec hdfs namenode"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9870/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus | grep -q 'active'"]
      interval: 10s
      timeout: 5s
      retries: 20
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1GB

  datanode:
    container_name: datanode_container
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-datanode
    hostname: datanode
    environment:
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      CLUSTER_NAME: hadoop-cluster
      TZ: America/Mexico_City
    networks:
      - bigdata_network
    volumes:
      - ./docker/compose/configs:/opt/hadoop/etc/hadoop:ro
      - hadoop_datanode:/hadoop/dfs/data
    #PRUEBA
    ports:
      - "9864:9864"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1GB
    command: >
      bash -c "
      while ! curl -sf http://namenode:9870/ >/dev/null; do
        echo 'Esperando a NameNode...';
        sleep 5;
      done;
      exec hdfs datanode"
    restart: unless-stopped
    depends_on:
      namenode:
        condition: service_healthy

  resourcemanager:
    container_name: resourcemanager_container
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-resourcemanager
    hostname: resourcemanager
    environment:
      SKIP_CONF_GENERATION: "true"
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      YARN_CONF_DIR: /opt/hadoop/etc/hadoop
      CLUSTER_NAME: hadoop-cluster
      TZ: America/Mexico_City
    networks:
        - bigdata_network
    entrypoint: ["bash", "-c", "exec yarn resourcemanager"]  # ðŸ‘ˆ Anula el entrypoint original
    volumes:
      - ./docker/compose/configs:/opt/hadoop/etc/hadoop:ro
      - hadoop_yarn_logs:/hadoop/yarn/logs
    ports:
      - "8088:8088"  # Web UI
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://resourcemanager:8088/ws/v1/cluster/info || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
    depends_on:
      - namenode
      - datanode
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2GB

  nodemanager:
    container_name: nodemanager_container
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-resourcemanager
    hostname: nodemanager
    environment:
      SKIP_CONF_GENERATION: "true" #Verificar
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      YARN_CONF_DIR: /opt/hadoop/etc/hadoop
      CLUSTER_NAME: hadoop-cluster
      TZ: America/Mexico_City
    networks:
      - bigdata_network
    entrypoint: ["bash", "-c", "exec yarn nodemanager"]
    volumes:
      - ./docker/compose/configs:/opt/hadoop/etc/hadoop:ro
      - hadoop_yarn_logs:/hadoop/yarn/logs
    ports:
      - "8042:8042"
    depends_on:
      - resourcemanager
    deploy:
      resources:
        limits:
          cpus: '6'
          memory: 12GB

  spark:
    container_name: spark_container
    build:
      context: .
      dockerfile: ./docker/files/Dockerfile-spark
    hostname: spark
    environment:
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      - HADOOP_CONF_DIR=/opt/bitnami/spark/conf
      - YARN_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_MASTER=yarn
      - SPARK_SUBMIT_OPTIONS="--conf spark.jars.ivy=/tmp/.ivy2"
      - TZ=America/Mexico_City
    networks:
      - bigdata_network
    volumes:
      - ./docker/compose/configs:/opt/bitnami/spark/conf:rw
      - ~/bigdata_platform/scripts:/opt/scripts
      - ~/bigdata_platform/jars:/opt/jars
    ports:
      - "4040:4040"
      - "7077:7077"
    healthcheck:
      test: ["CMD-SHELL", "netstat -tuln | grep 4040 || exit 1"]
      interval: 10s
      timeout: 2s
      retries: 10
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4GB
    command: >
      bash -c "
      until ping -c1 resourcemanager &>/dev/null; do
        echo 'Esperando a ResourceManager...';
        sleep 5;
      done;

      /opt/bitnami/spark/sbin/start-master.sh &&
      /opt/bitnami/spark/sbin/start-worker.sh spark://spark:7077 &&

      echo 'Configuraciones terminadas';
      tail -f /dev/null"


volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_yarn_logs:  
  spark_logs:
